# Рабочий пайплайн

Документ описывает полный алгоритм работы приложения «video-transcriber» во всех режимах: от загрузки конфигурации до выдачи результатов транскрипции, пост-обработки через LM Studio и пост-перевода субтитров.

## Архитектура и ключевые компоненты
- **PyQt6-приложение**: общий стек для GUI и CLI. Даже в консольном режиме создаётся `QCoreApplication`, потому что тяжелые операции выполняются в потоке `TranscriptionWorker`, наследнике `QThread`.
- **Конфигурация** (`app/config.py`): класс `AppConfig` читает и сохраняет JSON-файл настроек (`~/.video-transcriber/config.json`). Включает миграции (`LEGACY_KEY_MAP`), значения по умолчанию и автоматическое сохранение при изменении.
- **Транскрипционный движок** (`app/worker.py`): класс `TranscriptionWorker` управляет очередью задач, загрузкой моделей Whisper, извлечением аудио, пост-обработкой, записью результата и выстраивает конвейер коррекции с LM Studio.
- **Клиент LM Studio** (`app/lmstudio_client.py`): низкоуровневый HTTP-клиент, работающий с REST-совместимым API LM Studio. Умеет подбирать рабочий endpoint, загружать модель, переписывать и переводить батчи строк, учитывая лимиты токенов.
- **CLI-обёртка** (`app/cli.py`): парсер аргументов и удобный вызов `TranscriptionWorker` для одиночных задач без запуска GUI.
- **GUI** (`ui/main_window.py`): визуальный интерфейс, подключающийся к тем же воркерам (`TranscriptionWorker`, `TranslationWorker`) и управляющий настройками.
- **Перевод субтитров** (`app/translator.py`): отдельный поток `TranslationWorker` для пакетного перевода готовых файлов (.srt, .txt) через LM Studio.

## Конфигурация и загрузка
1. **Путь к конфигурации**: по умолчанию `~/.video-transcriber/config.json`. CLI позволяет передать альтернативный путь через `--config`.
2. **Загрузка**: `AppConfig.load_config()` читает JSON, объединяет с `defaults`. Ошибочные или отсутствующие файлы заменяются копией defaults.
3. **Сохранение**: `AppConfig.set()` обновляет значение ключа и сразу сохраняет файл. Папка создаётся рекурсивно.
4. **Поддержка наследия**: `_migrate_settings` переносит устаревшие ключи (g4f → local_llm_*), нормализует формат выходных файлов, форсирует `whisper_backend=faster`.

## Режимы запуска приложения
### GUI
1. Функция `main.launch_gui()` (из `main.py`) импортирует `torch` до подключения PyQt (Windows fix для CUDA).
2. Создаётся `QApplication`, загружается `AppConfig`.
3. Показывается `SplashScreen`, параллельно инициализируется `MainWindow(config)`.
4. Через `QTimer` splash закрывается, открывается основное окно. GUI управляет очередью задач, настройками и отображает логи воркера.

### CLI
1. Точка входа `main.main()` анализирует аргументы. Без аргументов или с `gui` запускается GUI.
2. Команда `transcribe` вызывает `app.cli.run_cli()`.
3. Parser (`build_parser`) поддерживает аргументы: путь к файлу, формат вывода, язык, модель, устройство, параметры LM Studio и т.д.
4. `transcribe_file()` собирает `ProcessingSettings`, формирует `TranscriptionTask`, стартует `TranscriptionWorker` и обрабатывает сигналы до завершения задачи.

## Формирование задач и настроек
- **ProcessingSettings** (текущие глобальные параметры пайплайна):
  - `pipeline_mode`: экземпляр строкового режима (`balanced`/`serial`/`staged`).
  - Лимиты параллелизма (`max_parallel_transcriptions`, `max_parallel_corrections`), пока сохраняются в задачах, но напрямую в воркере не используются.
  - Параметры Whisper (`release_whisper_after_batch`, `enable_cudnn_benchmark`).
  - Флаги глубокой коррекции, настройки LM Studio (URL, модель, токен-лимиты, таймауты).
- **TranscriptionTask**: конкретная работа для воркера. Хранит путь к входному файлу, директорию вывода, формат, язык, выбранный Whisper backend, устройство, список `OutputRequest` и все настройки LM Studio/коррекции.
- **TranslationTask**: пакет перевода готовых файлов. Включает языки (source/target), параметры LM Studio и контроль токенов.

## Жизненный цикл транскрипционной задачи
1. **Постановка в очередь**: GUI или CLI вызывает `TranscriptionWorker.add_task(task)`, задача попадает в `tasks_queue`.
2. **Основной цикл** (`TranscriptionWorker.run`):
   - Следит за флагом паузы.
   - Извлекает задачу и определяет контекст (`_choose_context_for_task`): `cpu`, `cuda`, `hybrid`, `auto`.
   - `hybrid` чередует CPU/GPU по загрузке очередей. `auto` предпочитает CUDA, если доступна.
3. **Потоки контекстов**:
   - Для каждого устройства (`_contexts`) поднимается отдельный поток `threading.Thread`, обрабатывающий очередь задач для своего устройства.
   - `release_whisper_after_batch` позволяет выгружать модель, когда очередь устройства пуста.
4. **Обработка задачи** (`_process_task_for_context`):
   - Проверка FFmpeg (`_ensure_ffmpeg_available`). Без бинарника транскрипция не стартует.
   - `progress_updated` → 5%. Логирует старт задачи и время.
   - **Загрузка модели** (`_ensure_model_for_context`):
     - Сначала пытается `faster-whisper` (если выбран). При неудаче или отсутствии пакета — откат к `openai-whisper`.
     - Учитывает предпочтительный девайс, при CUDA-ошибке автоматически переключается на CPU и перезапускает задачу.
     - Запоминает последнюю загруженную модель в контексте и переиспользует её для следующих задач.
   - `progress` → 15%.
   - **Извлечение аудио** (`_extract_audio`):
     - Если входной файл уже WAV — пропускает конверсию.
     - Предпочитает `ffmpeg_extract_audio` из moviepy; если нет или неудача, пробует `AudioFileClip`/`VideoFileClip`, затем fallback через CLI `ffmpeg`. На выходе — PCM WAV.
     - Запоминает временный файл для последующего удаления.
   - `progress` → 30%. Логирует имя временного аудио.
   - **Транскрипция** (`_transcribe`):
     - Faster-Whisper: VAD включён, beam_size=5, результат из генератора приводит к списку словарей `{start,end,text}`.
     - OpenAI Whisper: стандартный API `model.transcribe`.
     - При CUDA-ошибках и отсутствии CPU fallback выводится понятное сообщение.
     - Ошибки `invalid input features shape` инициируют переход с faster → openai.
   - **Диризация** (`_apply_speaker_diarization`): пока заглушка, возвращает исходные сегменты (поддержка в коде подготовлена).
   - `progress` → 70%.
   - **Переход к пост-обработке** (`_handle_post_transcription`):
     - Если коррекция отключена (`use_local_llm_correction=False`) или сегменты пустые — финализация.
     - Для режима `staged` сегменты сохраняются во временный JSON, задача добавляется в `_staged_corrections`, а коррекция откладывается до конца транскрипционного этапа.
     - Для остальных режимов вызывается `_dispatch_correction`.

## Режимы пайплайна (pipeline_mode)
- **balanced (по умолчанию)**:
  - Как только транскрипция завершена, сегменты отправляются в очередь коррекции.
  - Потоки транскрипции продолжают обрабатывать новые задачи параллельно с коррекцией в отдельном потоке.
- **serial**:
  - `_dispatch_correction` ставит `threading.Event` и блокирует обработку до завершения коррекции конкретной задачи.
  - Гарантируется строгая последовательность: транскрипция → коррекция → следующая задача.
- **staged**:
  - Сегменты сериализуются в JSON (TempDir `video-transcriber-staged`). В списке `_staged_corrections` копятся все задачи, требующие пост-обработки.
  - `_maybe_start_correction_phase` отслеживает активные транскрипции. Как только все очереди пусты и нет новых задач, начинается единая фаза коррекции: задачи массово передаются в `_dispatch_correction`.
  - После корректировки все временные файлы удаляются (`_cleanup_staged_transcript`).

## Очередь коррекции и взаимодействие с LM Studio
1. **Поток коррекции** (`_correction_worker`):
   - Работает в отдельном daemon-потоке, слушает `_correction_queue`.
   - Повторно использует `LmStudioClient` для задач с одинаковым `(base_url, model, api_key, token limits, timeouts)`.
2. **Проверки перед запросом**:
   - Если `use_local_llm_correction=False` или сегменты пусты — сразу финализация.
   - Если `use_lmstudio=False` или отсутствует URL/модель — логирует ошибку и выдаёт оригинальный текст.
3. **Создание клиента** (`_build_lmstudio_client`):
   - Формирует `LmStudioSettings` с лимитами токенов, температурой, таймаутами (по умолчанию 120 секунд).
   - `candidate.ensure_model_loaded()` вызывает REST `/models`, подбирает подходящий endpoint (`/models`, `/models/load`, `/internal/model/load`). Журналирует ход загрузки.
4. **Распределение батчей**:
   - `chunked()` группирует строки по `batch_size` (по умолчанию 40) и по токен-бюджету (`prompt_limit - token_margin`).
   - Слишком длинные строки отправляются по одной, чтобы не терять содержимое.
5. **Режим коррекции**:
   - `deep_correction` включает альтернативный системный prompt (`REWRITE_DEEP_SYSTEM_PROMPT`), разрешающий свободные переформулировки.
   - Стандартный режим (`polish`) выравнивает грамматику без изменения смысла и структуры.
6. **Вызов LM Studio**:
   - `rewrite_batch` формирует сообщение пользователя с секциями `Instructions`, `Task`, `Input`.
   - `_post()` циклически перебирает endpoints (`/chat/completions` на базовом URL с или без `/v1`), запоминает успешный и исключает недоступные.
7. **Контроль результата**:
   - `_split_lines` сверяет количество строк. Если отличается >10%, возвращает исходные сегменты (с предупреждением). При меньшем расхождении строки объединяются/дублируются, чтобы восстановить исходное количество.
   - `progress` → 90%, затем `_finalize_task`.
8. **Обработка ошибок**:
   - Любые исключения LM Studio (`LmStudioError`, таймауты, HTTP-ошибки) логируются как `error`. В этом случае сохраняется исходный текст.
   - При сбое клиент сбрасывается, чтобы следующее обращение создало новую сессию.

## Сохранение результатов
1. **Формирование списка форматов**: `TranscriptionTask.outputs` заполняется из конфигурации или CLI. Каждый `OutputRequest` содержит формат (`srt`, `txt_plain`, `txt_ts`, `vtt`, произвольное расширение) и флаг таймкодов.
2. **Запись** (`_write_outputs`):
   - Базовое имя — `<output_dir>/<stem>`. Для одинаковых расширений добавляется суффикс `_n`.
   - `srt`: используется модуль `srt`, спикеры (если есть) добавляются как префикс `Speaker: текст`.
   - `txt_plain`: просто текст, блоки разделены пустыми строками. `txt_ts` снабжает каждую строку временной меткой `HH:MM:SS.mmm → текст`.
   - `vtt`: формат `WEBVTT` с миллисекундами.
   - Неизвестные форматы: plain text с учётом флага таймкодов.
3. **Финализация** (`_finalize_task`):
   - `task.result_paths` заполняется путями файлов, прогресс обновляется до 100%.
   - Эмитится сигнал `task_completed`, лог `success` с количеством файлов.
   - Сохраняется статистика по времени (`_log_task_timing`).
   - В случае ошибок записи — `task_failed`, лог `error`.
4. **Очистка**:
   - Временные WAV удаляются.
   - Для staged-режима очищаются JSON-файлы.
   - Если `release_whisper_after_batch=True`, после опустошения очереди контекста модель выгружается и очищается `torch.cuda.empty_cache()`.

## Пайплайн перевода субтитров
1. **Очередь**: `TranslationWorker` (QThread) содержит `tasks_queue`.
2. **Обработка**:
   - Проверяет `use_lmstudio`. Если отключено — выбрасывает ошибку.
   - Создаёт `LmStudioClient`, загружает модель (`ensure_model_loaded`).
   - В зависимости от расширения файла выбирает:
     - `.srt`: парсит `srt.parse`, формирует список реплик, отправляет батчами в `translate_batch`.
     - `.txt`: разбивает по строкам (пустые строки отбрасываются), переводит батчами.
   - Количество строк ответа должно совпадать с исходным. Несовпадение вызывает `LmStudioError`.
   - Сохраняет результат рядом с исходным файлом в формате `<stem>_<target_lang>.(srt|txt)`.
3. **Сигналы**:
   - `translation_completed(task_id, path)` по успеху.
   - `translation_failed(task_id, message)` при ошибке.
   - Логи (`log_message`) дублируют ход работы.

## Детали клиента LM Studio
- **Инициализация**:
  - Принимает базовый URL в любом виде: `http://127.0.0.1:1234/v1`, `http://127.0.0.1:1234`. Конструирует список корней (с `/v1` и без него).
  - Поддерживает несколько вариантов загрузки моделей (`/models`, `/models/load`, `/internal/model/load`).
  - Хранит сессию `requests.Session` с заголовками JSON и опциональным Bearer-токеном.
- **Отправка запросов** (`_post`):
  - Перебирает кандидатов, кеширует успешный endpoint в `_active_chat_endpoint`.
  - При HTTP-ошибке сохраняет исключение и пробует следующий.
  - Если ответ 200 содержит `{"error": ...}`, endpoint помечается как недоступный.
  - Таймаут запроса задаётся `settings.timeout` (по умолчанию 120 с).
- **Разбор ответов**:
  - `chat_completion` ожидает JSON OpenAI-совместимого вида и извлекает `choices[0].message.content`.
  - Ошибка декодирования JSON или отсутствие структуры приводит к `LmStudioError`.
- **Подготовка прому**:
  - `_compose_user_prompt` собирает входные секции с инструкциями, заданием и исходными строками.
  - `_estimate_tokens` использует простые эвристики по символам/словам.
  - `_completion_budget` масштабирует лимит выходных токенов, добавляет запас на 64 токена.
  - `chunked` гарантирует, что ни один батч не превысит разрешение по количеству строк и токенов.

## Управление ресурсами и завершение
- `TranscriptionWorker.stop_processing()` очищает все очереди, выгружает модели и временные файлы, но не завершает поток. Используется при паузе или закрытии приложения.
- `TranscriptionWorker.stop()` полностью завершает поток: выставляет `_shutdown_event`, отправляет `None` в очереди, ожидает завершение потоков устройств и коррекции.
- Тайминги (`_log_task_timing`) ведут статистику по длительности отдельных задач и общего времени.
- `clear_queue()` сбрасывает все накопленные staged-задачи и активные подсчёты.
- CUDA-зависимости: при первом запуске на Windows `_configure_cuda_dll_search_path()` добавляет пути к библиотекам torch/nvidia, чтобы Whisper обнаружил нужные DLL.

## Известные ограничения и особенности
- Параметры `max_parallel_transcriptions` и `max_parallel_corrections` пока не ограничивают число параллельных задач явно — инфраструктура подготовлена, но логика распределения ещё не реализована.
- Параметр `correction_device` (CPU/GPU) хранится в настройках для будущей локальной LLM, но на текущий момент не влияет на код.
- Блок `speaker diarization` пока заглушён; в файлах нет фактического выделения спикеров.
- Механизм `staged` предполагает достаточное дисковое пространство во временной директории и не содержит ретраев при ошибке записи.
- Переводчик (`TranslationWorker`) поддерживает только .srt и .txt (plain). Другие форматы нужно конвертировать вручную.
- LM Studio запросы используют единый таймаут 120 секунд. Для длительных генераций модель должна уметь отвечать быстрее или таймаут следует увеличить в конфиге.

Документ отражает поведение актуального кода и может использоваться как основа для пользовательской документации, ревью архитектуры и планирования доработок.
